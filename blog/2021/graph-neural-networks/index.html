<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Aryan  Chouhan | Graph Neural Networks</title>
<meta name="description" content="I'm a computer engineer who likes building stuff.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí≠</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/graph-neural-networks/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Aryan</span>   Chouhan
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Resume -->
          <li class="nav-item">
            <a class="nav-link" href="assets/pdf/resume.pdf">
              resume
            </a>
          </li>
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Graph Neural Networks</h1>
    <p class="post-description"></p>
    <p class="post-meta">July 28, 2021</p>
  </header>

  <hr>

  <article class="post-content">
    <h5 id="video-lectures---youtube-playlist-cs224w">Video lectures - <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn">YouTube playlist: CS224W</a></h5>
<h5 id="textbook---graph-representation-learning---william-l-hamilton">Textbook - <a href="https://www.cs.mcgill.ca/~wlh/grl_book/">Graph Representation Learning - William L. Hamilton</a></h5>
<h5 id="course---cs224w---prof-jure-leskovec">Course - <a href="https://cs224w.stanford.edu/">CS224W - Prof. Jure Leskovec</a></h5>
<hr />

<h2 id="day-1">Day 1</h2>

<h4 id="what-are-graphs">What are graphs?</h4>

<h4 id="machine-learning-tasks-on-graphs">Machine learning tasks on graphs</h4>
<ul>
  <li>Node classification</li>
  <li>Link (Relation) prediction</li>
  <li>‚Ä¶</li>
</ul>

<h4 id="node-level-features">Node-level features</h4>
<ul>
  <li>Node Degree</li>
  <li>Node Centrality
    <ul>
      <li>Eigenvector Centrality [a node is important if its neighbors are important] <em>(have to re-read - linear algebra concepts)</em></li>
      <li>Betweenness Centrality [a node is important if it is an important connector/bridge]</li>
      <li>Closeness Centrality [a node is important if it is close to many nodes]</li>
    </ul>
  </li>
  <li>Clustering Coefficient (local structure) [a node is important if its neighboring nodes are highly connected with each other]</li>
  <li>Graphlets (!)
    <ul>
      <li>Observation: Clustering coefficent = no. of triangles in the ego network of a nodes</li>
      <li>Generalize this notion by counting occurrences of different graphlets</li>
      <li>Graphlet Degree Vector (different from Graphlet Features - from graphlet kernel)</li>
    </ul>
  </li>
</ul>

<h4 id="link-level-features-yet-to-read">Link-level features <em>(yet to read)</em></h4>

<h4 id="graph-level-features">Graph-level features</h4>
<ul>
  <li>Kernel methods <em>(have to polish up on fundamentals of kernels)</em>
    <ul>
      <li>Graphlet kernel [Bag-of-Words for a graph]</li>
      <li>Weisfeiler-Lehman kernel [Bag-of-Colors/Generalized version of Bag-of-Words]
        <ul>
          <li>Isomorphism Test</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="node-embeddings">Node Embeddings</h4>
<ul>
  <li>Task: map nodes into an embedding space</li>
  <li>Properties of embeddings
    <ul>
      <li>Unsupervised/self-supervised learning (node labels/features are not utilized)</li>
      <li>Task-independent</li>
    </ul>
  </li>
  <li>‚ÄúShallow‚Äù Encoding
    <ul>
      <li>Process
        <ol>
          <li>Encoder - nodes ü°í embeddings [encoder = embedding lookup]</li>
          <li>Node similarity function (in original network)</li>
          <li>Decoder - embeddings ü°í similarity score (for the embeddings) [usually just the dot product = cosine distance]</li>
          <li>Optimize encoder parameters (node embeddings) such that \(similarity(u, v) \approx z_u^Tz_v\)</li>
        </ol>
      </li>
      <li>How to define node similarity?
        <ul>
          <li><a href="https://www.youtube.com/watch?v=Xv0wRy66Big">DeepWalk, node2vec</a></li>
        </ul>
      </li>
      <li>Link between Matrix Factorization and Node Embeddings - <a href="https://arxiv.org/abs/1710.02971">Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec</a></li>
      <li>Limitations (matrix factorization, random walks)
        <ul>
          <li>Cannot obtain embeddings for nodes not in the training set (all embeddings need to be recomputed - random walks)</li>
          <li>Cannot capture structural similarity (solved by anonymous walks - <em>yet to read</em>)</li>
          <li>Cannot utilize node, link, graph features</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="day-2">Day 2</h2>

<h4 id="message-passing">Message Passing</h4>
<ul>
  <li>Intuition: correlations exist in networks
    <ul>
      <li>Homophily [individual characteristics ü°í social connections]</li>
      <li>Influence [social connections ü°í individual characteristics]</li>
    </ul>
  </li>
</ul>

<h4 id="node-classification">Node classification</h4>
<ul>
  <li>Semi-supervised</li>
  <li>Given a graph (\(A = n \times n\) adjacency matrix) of \(n\) nodes with a few labeled nodes (\(Y = \{0, 1\}^n\) vector of labels), we wish to predict the labels of the remaining nodes</li>
  <li>Assumption: there is homophily in the network</li>
</ul>

<h4 id="collective-classification">Collective classification</h4>
<ul>
  <li>Probabilistic framework</li>
  <li>Markov assumption - label \(Y_v\) of one node \(v\) depends upon labels of only its neighboring nodes \(N_v\) (First order Markov assumption \(\implies\) degree 1 dependence)</li>
  <li>Basically, \(P(Y_v) = P(Y_v \vert N_v)\)</li>
  <li>‚ÄòCollective‚Äô - we will classify every node in the graph</li>
  <li>Steps
    <ul>
      <li>Local classifier</li>
      <li>Relational classifier</li>
      <li>Collective inference</li>
    </ul>
  </li>
</ul>

<h4 id="techniques-yet-to-read">Techniques <em>(yet to read)</em></h4>
<ul>
  <li>Relational classification</li>
  <li>Iterative classification</li>
  <li>Belief propagation</li>
</ul>

<h4 id="graph-neural-networks">Graph Neural Networks</h4>
<ul>
  <li>Limitations of shallow embeddings
    <ul>
      <li>No. of parameters is very high - \(O(\vert V \vert)\)</li>
      <li>‚Äòtransductive inference‚Äô - predictions cannot be made for examples not in the training set (as opposed to inductive inference)</li>
      <li>Do not incorporate node features</li>
    </ul>
  </li>
  <li>Deep Graph Encoders
    <ul>
      <li>Here, \(ENC(v)\) = neural network</li>
      <li>Graph ü°í Graph convolutions ü°í Activations ü°í Regularization ü°í ‚Ä¶ ü°í Node embeddings</li>
    </ul>
  </li>
  <li>Deep Learning for Graphs
    <ul>
      <li>Naive approach - adjacency matrix + features = input to network
        <ul>
          <li>\(O(\vert V \vert)\) parameters</li>
          <li>Graph sizes</li>
          <li>Sensitive to node ordering</li>
        </ul>
      </li>
      <li>Better approach - generalize convolutions (from CNNs) beyond simple lattices to allow it to leverage node features (eg. text, images)
        <ul>
          <li>Very tough
            <ul>
              <li>No notion of locality or sliding window for graphs</li>
              <li>Graphs are permutation invariant</li>
            </ul>
          </li>
          <li>What do we do?
            <ul>
              <li>Image filters = information from neighboring pixels</li>
              <li>Graph convolutions = information from neighboring nodes = determine node computation graph + propagate and transform information</li>
              <li>This builds on message passing (see above)</li>
              <li>Every node defines a different computation graph based on its neighborhood</li>
              <li>Training over every neural network, not just one</li>
              <li>K layer net for K neighborhood steps</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="day-3">Day 3</h2>

<h4 id="graph-neural-networks-1">Graph Neural Networks</h4>
<ul>
  <li>Deep Learning for Graphs
    <ul>
      <li>Basic GNN structure</li>
      <li>Aggregate and Update functions</li>
      <li>Depending upon different ways of doing this - different models</li>
      <li>Problems also possible: over-smoothing</li>
      <li>Graph Convolution Networks (GCN), Graph Attention Networks (GAT), GraphSAGE</li>
      <li>Concatenations, skip connections</li>
    </ul>
  </li>
  <li>GNNs = WL algorithm (for discrete node features)
    <ul>
      <li>Active area of research - upgrade GNNs to be provably stronger</li>
    </ul>
  </li>
</ul>

<!-- ## Day 4

#### DEMO-Net
- Current GNNs pitfalls identified
    - graph convolutions (aggregation function) properties = seed-oriented + degree-aware + order-free
        - (to be clear, these are properties that we want but most GNN approaches do not have them)
    - degree information is not optimally utilized
    - graph-level pooling schemes theoretically unclear (?)
- Proposal: *multi-task graph convolution where each task represents node representation learning for nodes with a specific degree value, thus leading to preserving the degree specific graph structure*
- task = learning node embeddings for all nodes with a specific degree  
  multi-task = learning node embeddings for all nodes with all degrees
- Proposed Method: *degree-specific weight and hashing functions for graph convolutions*
- WL algorithm-inspired design
    - Based on properties (see above)
    - Mathematically equivalent as well (?)
- Idea/Assumption: *nodes with same degrees share the same graph convolutions*
    - Why: reasoning is that nodes with same degrees are structurally similar & have similar 'roles' (note: subtree structures)
- My understanding
    - Understood most of the proposed design
    - Equation 5 $$ \checkmark $$
    - Degree-specific weight function $$ \checkmark $$
        - Idea: incorporating attention into degree specificity
    - Hashing function
        - A bit unclear
            - Purpose: Since the no. of nodes is a lot, we hash it and hence, same nodes get the same representations down the line?
            - Discrete vs continuous
        - Analogy to WL algorithm
    - Node $$ \checkmark $$ vs Graph $$ \chi $$ -->

<h4 id="tensorflow">Tensorflow</h4>
<ul>
  <li>Placeholders, Sessions</li>
</ul>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    <!-- 
<footer class="fixed-bottom">
  <div style="text-align: center;" class="container mt-0">
    &copy; Copyright 2021 Aryan  Chouhan.
    
    
    
  </div>
</footer>

 -->

  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
